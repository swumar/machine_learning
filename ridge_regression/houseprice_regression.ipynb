{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 4 - Linear versus Ridge Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your friend Bob just moved to Boston. He is a real estate agent who is trying to evaluate the prices of houses in the Boston area. He has been using a linear regression model but he wonders if he can improve his accuracy on predicting the prices for new houses. He comes to you for help as he knows that you're an expert in machine learning. \n",
    "\n",
    "As a pro, you suggest doing a *polynomial transformation*  to create a more flexible model, and performing ridge regression since having so many features compared to data points increases the variance. \n",
    "\n",
    "Bob, however, being a skeptic isn't convinced. He wants you to write a program that illustrates the difference in training and test costs for both linear and ridge regression on the same dataset. Being a good friend, you oblige and hence this assignment :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you are to explore the effects of ridge regression.  We will use a dataset that is part of the sklearn.dataset package.  Learn more at https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:  Getting, understanding, and preprocessing the dataset\n",
    "\n",
    "We first import the standard libaries and some libraries that will help us scale the data and perform some \"feature engineering\" by transforming the data into $\\Phi_2({\\bf x})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the boston dataset from sklearn\n",
    "boston_data = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of features is:  14\n",
      "The features:  ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "The number of exampels in our dataset:  506\n",
      "[[1.0000e+00 6.3200e-03 1.8000e+01 2.3100e+00 0.0000e+00 5.3800e-01\n",
      "  6.5750e+00 6.5200e+01 4.0900e+00 1.0000e+00 2.9600e+02 1.5300e+01\n",
      "  3.9690e+02 4.9800e+00]\n",
      " [1.0000e+00 2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01\n",
      "  6.4210e+00 7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01\n",
      "  3.9690e+02 9.1400e+00]]\n"
     ]
    }
   ],
   "source": [
    "#  Create X and Y variables - X holding the .data and Y holding .target \n",
    "X = boston_data.data\n",
    "y = boston_data.target\n",
    "\n",
    "#  Reshape Y to be a rank 2 matrix \n",
    "y = y.reshape(X.shape[0], 1)\n",
    "\n",
    "# Proprocesing by adding a column of 1's to the front of X\n",
    "one_col = np.ones((X.shape[0],1))\n",
    "X = np.hstack((one_col, X))\n",
    "\n",
    "# Observe the number of features and the number of labels\n",
    "print('The number of features is: ', X.shape[1])\n",
    "# Printing out the features\n",
    "print('The features: ', boston_data.feature_names)\n",
    "# The number of examples\n",
    "print('The number of exampels in our dataset: ', X.shape[0])\n",
    "#Observing the first 2 rows of the data\n",
    "print(X[0:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also create polynomial features for the dataset to test linear and ridge regression on data with degree = 1 and data with degree = 2. Feel free to increase the # of degress and see what effect it has on the training and test error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PolynomialFeatures object with degree = 2. \n",
    "# Transform X and save it into X_2. Simply copy Y into Y_2 \n",
    "# Note: PolynomialFeatures creates a column of ones as the first feature\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_2 = poly.fit_transform(X)\n",
    "y_2 = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 120)\n",
      "(506, 1)\n"
     ]
    }
   ],
   "source": [
    "# the shape of X_2 and Y_2 - should be (506, 105) and (506, 1) respectively\n",
    "print(X_2.shape)\n",
    "print(y_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Define the get_coeff_ridge_normaleq function. Use the normal equation method.\n",
    "# TODO - Return w values\n",
    "\n",
    "def get_coeff_ridge_normaleq(X_train, y_train, alpha):\n",
    "    # use np.linalg.pinv(a)\n",
    "    #### TO-DO #####\n",
    "    \n",
    "    w = np.linalg.lstsq(X_train.T.dot(X_train)+(alpha * np.identity(X_train.shape[1])),X_train.T.dot(y_train),rcond=None)[0]\n",
    "    \n",
    "    ##############\n",
    "    return w\n",
    "\n",
    "def error(x, y, w):\n",
    "\n",
    "    p = np.dot(x , w) - y\n",
    "    err = np.dot(np.transpose(p),p) / x.shape[0]\n",
    "    \n",
    "    return err[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Define the evaluate_err_ridge function.\n",
    "# TODO - Return the train_error and test_error values\n",
    "\n",
    "\n",
    "def evaluate_err(X_train, X_test, y_train, y_test, w): \n",
    "    #### TO-DO #####\n",
    "    \n",
    "\n",
    "    train_error = error(X_train, y_train, w)\n",
    "    test_error = error(X_test, y_test, w)\n",
    "    \n",
    "    \n",
    "    ##############\n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Finish writting the k_fold_cross_validation function. \n",
    "# TODO - Returns the average training error and average test error from the k-fold cross validation\n",
    "# use Sklearns K-Folds cross-validator: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "\n",
    "def k_fold_cross_validation(k, X, y, alpha):\n",
    "    kf = KFold(n_splits=k, random_state=21, shuffle=True)\n",
    "    total_E_val_test = 0\n",
    "    total_E_val_train = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # scaling the data matrix (except for for the first column of ones)\n",
    "        scaler = preprocessing.StandardScaler().fit(X_train[:,1:(X_train.shape[1]+1)])\n",
    "        X_train[:,1:(X_train.shape[1]+1)] = scaler.transform(X_train[:,1:(X_train.shape[1]+1)])\n",
    "        X_test[:,1:(X_train.shape[1]+1)] = scaler.transform(X_test[:,1:(X_train.shape[1]+1)])\n",
    "    \n",
    "        \n",
    "        # determine the training error and the test error\n",
    "        #### TO-DO #####\n",
    "    \n",
    "        w = get_coeff_ridge_normaleq(X_train,y_train,alpha)\n",
    "        train_error, test_error = evaluate_err(X_train, X_test, y_train , y_test, w)\n",
    "        total_E_val_test += test_error\n",
    "        total_E_val_train += train_error\n",
    "       ##############\n",
    "\n",
    "    E_val_test = total_E_val_test / k\n",
    "    E_val_train = total_E_val_train / k\n",
    "    \n",
    "    \n",
    "    \n",
    "       ##############\n",
    "    return  E_val_test, E_val_train\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without tranformation\n",
      "Alpha: 0.00000    Test_MSE: 23.63607    Train_MSE: 21.80618\n",
      "Alpha: 1.02329    Test_MSE: 23.63511    Train_MSE: 21.81006\n",
      "Alpha: 1.23737    Test_MSE: 23.63588    Train_MSE: 21.81183\n",
      "Alpha: 1.49624    Test_MSE: 23.63726    Train_MSE: 21.8144\n",
      "Alpha: 1.80926    Test_MSE: 23.63956    Train_MSE: 21.81813\n",
      "Alpha: 2.18776    Test_MSE: 23.64325    Train_MSE: 21.82353\n",
      "Alpha: 2.64545    Test_MSE: 23.649    Train_MSE: 21.83133\n",
      "Alpha: 3.1989    Test_MSE: 23.65782    Train_MSE: 21.84259\n",
      "Alpha: 3.86812    Test_MSE: 23.67111    Train_MSE: 21.8588\n",
      "Alpha: 4.67735    Test_MSE: 23.69092    Train_MSE: 21.88208\n",
      "Alpha: 5.65588    Test_MSE: 23.72015    Train_MSE: 21.91543\n",
      "Alpha: 6.83912    Test_MSE: 23.76289    Train_MSE: 21.96308\n",
      "Alpha: 8.2699    Test_MSE: 23.82492    Train_MSE: 22.03095\n",
      "Alpha: 10.0    Test_MSE: 23.91436    Train_MSE: 22.12732\n",
      "With polynomial transformation of degree 2:\n",
      "Alpha: 0.00000    Test_MSE: 11.85497    Train_MSE: 5.80882\n",
      "Alpha: 1.02329    Test_MSE: 11.44686    Train_MSE: 7.27939\n",
      "Alpha: 1.23737    Test_MSE: 11.52326    Train_MSE: 7.41831\n",
      "Alpha: 1.49624    Test_MSE: 11.60889    Train_MSE: 7.56754\n",
      "Alpha: 1.80926    Test_MSE: 11.70451    Train_MSE: 7.72824\n",
      "Alpha: 2.18776    Test_MSE: 11.81122    Train_MSE: 7.90176\n",
      "Alpha: 2.64545    Test_MSE: 11.93066    Train_MSE: 8.0898\n",
      "Alpha: 3.1989    Test_MSE: 12.06516    Train_MSE: 8.29451\n",
      "Alpha: 3.86812    Test_MSE: 12.21799    Train_MSE: 8.51873\n",
      "Alpha: 4.67735    Test_MSE: 12.39363    Train_MSE: 8.76618\n",
      "Alpha: 5.65588    Test_MSE: 12.59793    Train_MSE: 9.04177\n",
      "Alpha: 6.83912    Test_MSE: 12.83836    Train_MSE: 9.35186\n",
      "Alpha: 8.2699    Test_MSE: 13.12421    Train_MSE: 9.70458\n",
      "Alpha: 10.0    Test_MSE: 13.46678    Train_MSE: 10.11024\n"
     ]
    }
   ],
   "source": [
    "print(\"Without tranformation\")\n",
    "\n",
    "#Linear regression with alpha = 0 and degree 1:\n",
    "test_mse, train_mse = k_fold_cross_validation(10,X,y,0)\n",
    "print(\"Alpha: 0.00000\",\"  \",\"Test_MSE:\",round(test_mse,5),\"  \",\"Train_MSE:\",round(train_mse,5))\n",
    "\n",
    "# Ridge_regression with given alphas and degree 1:\n",
    "alphas = np.logspace(.01, 1, num=13)\n",
    "for i in alphas:\n",
    "    test_mse, train_mse = k_fold_cross_validation(10,X,y,i)\n",
    "    print(\"Alpha:\",round(i,5),\"  \",\"Test_MSE:\",round(test_mse,5),\"  \",\"Train_MSE:\",round(train_mse,5))\n",
    "\n",
    "\n",
    "print(\"With polynomial transformation of degree 2:\")\n",
    "\n",
    "#Linear regression with alpha = 0 and degree 2:\n",
    "test_mse, train_mse = k_fold_cross_validation(10,X_2,y_2,0)\n",
    "print(\"Alpha: 0.00000\",\"  \",\"Test_MSE:\",round(test_mse,5),\"  \",\"Train_MSE:\",round(train_mse,5))\n",
    "\n",
    "# Ridge_regression with given alphas and degree 2:\n",
    "alphas = np.logspace(.01, 1, num=13)\n",
    "for i in alphas:\n",
    "    test_mse, train_mse = k_fold_cross_validation(10,X_2,y_2,i)\n",
    "    print(\"Alpha:\",round(i,5),\"  \",\"Test_MSE:\",round(test_mse,5),\"  \",\"Train_MSE:\",round(train_mse,5))\n",
    "#Predicting the scaled price of the house\n",
    "\n",
    "# test = np.array([[1, 5, 0.5, 2, 0, 4, 8, 4, 6, 2, 2, 2, 4, 5.5]])\n",
    "# scaler = preprocessing.StandardScaler().fit(X[:,1:(X.shape[1]+1)]) #Scaled the entire data\n",
    "# X[:,1:(X.shape[1]+1)] = scaler.transform(X[:,1:(X.shape[1]+1)])\n",
    "# test[:, 1:(X.shape[1] + 1)] = scaler.transform(test[:, 1:(X.shape[1] + 1)]) #Scaling the given input\n",
    "# w = get_coeff_ridge_normaleq(X,y,1.02329299) #using the best alpha\n",
    "# print(w.T.dot(test.T))\n",
    "# print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
